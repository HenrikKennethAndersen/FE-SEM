---
title: "Supplememtary examples for:"
#author: "Henrik Kenneth Andersen"
#date: "`r Sys.Date()`"
output: 
  pdf_document:
    fig_caption: yes
    number_sections: yes
    includes: 
      in_header: header.tex
documentclass: article
classoption: a4paper
bibliography: references.bib   
---

\pagenumbering{arabic} 
\setcounter{page}{1}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# ----- Packages 

library( formatR)
library( knitr)
library( lavaan)

source("scripthooks.R")
```

Andersen, Henrik K. (2020). A closer look at fixed-effects regression in structural equation modeling using lavaan. 

```{r echo=FALSE}
df <- readRDS("longData.Rda")
dfw <- readRDS("wideData.Rda")

n <- readRDS("n.Rda")
waves <- readRDS("waves.Rda")

fe_sem <- readRDS("fe_sem.Rda")
fe_sem.fit <- readRDS("fe_sem.fit.Rda")
```

# Extensions {#exten}

The following discusses several opportunities to extend the basic FE model outlined in the main article, and provides concrete guidance on the implementation in `lavaan`. First, I go over a number of possibilities to relax assumptions associated with the traditional FE model. Then, I discuss the issue of measurement error and show how we can use latent variables to deal with it and allow us to properly estimate the coefficients of interest. Then, I show display a type of hybrid FE/RE model that allows us to control for time-invariant unobserved heterogeneity that allows us to include time-invariant predictors in the model.   

## Relaxing assumptions meant to mimic traditional FE models {#relax}

There are a number of implicit assumptions attached to the typical FE model that can be relaxed in SEM. Some of these assumptions have been discussed already, and a full list of assumptions can be found in @Bollen2010. Here, I will go over just a few, concentrating on the implementation in `lavaan` and the opportunity to empirically test whether the adjustments are justified or not. 

The assumptions we will discuss here pertain to the time-invariance of the effects of both the latent individual effects and the observed covariates, as well as a time-invariant error variance. We can also empirically test the correlation between the individual effects and the covariates to see whether a RE model is preferable to the FE model. 

For example, we can rewrite the original FE equation as
\begin{align}
y_{it} & = \beta_{t}x_{it} + \lambda_{t}\alpha_{i} + \varepsilon_{it}
\end{align}
where $\beta$ becomes $\beta_{t}$ and the implicit regression weight of one turns to $\lambda_{t}$ to highlight the fact that the effect of $x$ as well as $\alpha$ on $y$ may vary over time. We can furthermore easily relax the assumption of time-constant error variance, i.e., $\sigma^{2}_{\varepsilon_{t}}$. As noted above, the assumption regarding $\E[\alpha x_{t}]$ in $\bm{\Psi}$ determines whether we have an FE or RE model. We can set these to zero and test whether the RE model would be preferable to the FE model. In general, if the individual effects are truly uncorrelated with the model covariates, it is advisable to switch to an RE model since because it uses up less degrees of freedom, it will have smaller asymptotic standard errors [@Bollen2010]. 

In the following `lavaan` code, we simply remove the factor loadings of one for the latent individual effect variable which allows them to be estimated freely at each timepoint. For the effect of the covariate, we can either delete the constraints `b` in `yt ~ b*xt` or give each regression a different label, e.g., `b1`, `b2`, `b3`, etc. Similarly, to allow the error variance to vary over time, we turn the constraints `e` into simple labels, i.e., `e1`, `e2`, `e3`, etc., or again just delete them. In fact, regarding the error variances, they will be estimated necessarily, and do not need to be explicitly mentioned in the model syntax at all. Finally, to move from an FE to an RE model, we could simply constrain the correlations between the individual effects and the covariates to zero, i.e., `a ~~ 0*x1 + 0*x2 + 0*x3 + 0*x4 + 0*x5`. 

```{r eval=FALSE}
fe_sem_fullyrelaxed <- '
# Define individual effects variable 
a =~ y1 + y2 + y3 + y4 + y5
# Regressions, constrain coefficient to be equal over time
y1 ~ b1*x1
y2 ~ b2*x2 
y3 ~ b3*x3
y4 ~ b4*x4
y5 ~ b5*x5
# Allow unrestricted correlation between eta and covariates
a ~~ x1 + x2 + x3 + x4 + x5
# Alternatively: constrain all to 0 for RE model, or
# just individual correlations
# a ~~ 0*x1 + 0*x2 + 0*x3 + 0*x4 + 0*x5
x1 ~~ x2 + x3 + x4 + x5
x2 ~~ x3 + x4 + x5
x3 ~~ x4 + x5
x4 ~~ x5
# Constrain residual variances to be equal over time
y1 ~~ e1*y1
y2 ~~ e2*y2
y3 ~~ e3*y3
y4 ~~ e4*y4
y5 ~~ e5*y5
'
fe_sem_fullyrelaxed.fit <- sem( model = fe_sem_fullyrelaxed, 
                                data = dfw, 
                                estimator = "ML")
```

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
re_sem <- '
# Define individual effects variable 
a =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5
# Regressions, constrain coefficient to be equal over time
y1 ~ b*x1
y2 ~ b*x2 
y3 ~ b*x3
y4 ~ b*x4
y5 ~ b*x5
# Allow unrestricted correlation between eta and covariates
a ~~ 0*x1 + 0*x2 + 0*x3 + 0*x4 + 0*x5
x1 ~~ x2 + x3 + x4 + x5
x2 ~~ x3 + x4 + x5
x3 ~~ x4 + x5
x4 ~~ x5
# Constrain residual variances to be equal over time
y1 ~~ e*y1
y2 ~~ e*y2
y3 ~~ e*y3
y4 ~~ e*y4
y5 ~~ e*y5
'
re_sem.fit <- sem( model = re_sem, 
                   data = dfw, 
                   estimator = "ML")
```

As outlined in the @Bollen2010 text, the researcher has the opportunity to test each of the assumptions empirically and decide whether a more parsimonious, i.e., restrictive model is justifiable. For each assumption, a likelihood ratio test can be carried out to determine whether the improvement to model fit resulting from the relaxation of various assumptions is significant or whether the more parsimonious model is preferable after all. 

If we use the original model `fe_sem.fit` as a starting point, the best strategy for testing these assumptions is to work in a stepwise fashion, relaxing one assumption at a time. We can begin by first constraining the correlation between $\alpha$ and $x_{t}$ to zero (`re_sem`) for an RE model. If turning from an FE to an RE model does not significantly worsen model fit, we can go forward with the rest of the steps with the RE model. If, however, the fit does worsen significantly, it is likely better to stick with the FE model; moving forward then with it to see if a less restrictive FE model is preferable. We can perform a likelihood ratio test in `R` using the `anova()` function:

\small
```{r}
anova( fe_sem.fit, re_sem.fit)
```
\normalsize

The table that is generated shows a comparison of the nested models, in decending order according to degrees of freedom. The RE model does not estimate the correlations between the individual effects and the covariates, so it is more parsimonious and thus listed at the bottom. The `Chisq` column shows the $\chi^{2}$ statistic for both models and the `Chisq diff` column calculates the difference between the two. Obviously, according to the DGP, the correlation between the individual effects and $x_{t}$ is not zero, so fixing these to zero leads to a substantial amount of misfit. The last column puts the $\chi^{2}$ difference in relation to the difference in degrees of freedom and gives a p-value for the probability that the difference is solely due to chance. Here, the change in $\chi^{2}$ is highly significant, so the FE model should be retained. 

After now having established once and for all that FE is our preferred model, we can begin relaxing the rest of the assumptions. I show the following merely as a demonstration of the procedure, we know already from the DGP that the parsimonious model as specified in `fe_sem.fit` is appropriate. We can next allow the error variances (`fe_semb.fit`), the effect of $x$ on $y$ (`fe_semc.fit`) and finally the factor loadings of the individual effects (`fe_semd.fit`) all to vary over time. 

```{r echo=FALSE}
fe_semb <- '
# Define individual effects variable 
a =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5
# Regressions, constrain coefficient to be equal over time
y1 ~ b*x1
y2 ~ b*x2 
y3 ~ b*x3
y4 ~ b*x4
y5 ~ b*x5
# Allow unrestricted correlation between eta and covariates
a ~~ x1 + x2 + x3 + x4 + x5
x1 ~~ x2 + x3 + x4 + x5
x2 ~~ x3 + x4 + x5
x3 ~~ x4 + x5
x4 ~~ x5
# Constrain residual variances to be equal over time
y1 ~~ e1*y1
y2 ~~ e2*y2
y3 ~~ e3*y3
y4 ~~ e4*y4
y5 ~~ e5*y5
'
fe_semb.fit <- sem( model = fe_semb, 
                   data = dfw, 
                   estimator = "ML")

fe_semc <- '
# Define individual effects variable 
a =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5
# Regressions, constrain coefficient to be equal over time
y1 ~ b1*x1
y2 ~ b2*x2 
y3 ~ b3*x3
y4 ~ b4*x4
y5 ~ b5*x5
# Allow unrestricted correlation between eta and covariates
a ~~ x1 + x2 + x3 + x4 + x5
x1 ~~ x2 + x3 + x4 + x5
x2 ~~ x3 + x4 + x5
x3 ~~ x4 + x5
x4 ~~ x5
# Constrain residual variances to be equal over time
y1 ~~ e1*y1
y2 ~~ e2*y2
y3 ~~ e3*y3
y4 ~~ e4*y4
y5 ~~ e5*y5
'
fe_semc.fit <- sem( model = fe_semc, 
                   data = dfw, 
                   estimator = "ML")

fe_semd <- '
# Define individual effects variable 
a =~ y1 + y2 + y3 + y4 + y5
# Regressions, constrain coefficient to be equal over time
y1 ~ b1*x1
y2 ~ b2*x2 
y3 ~ b3*x3
y4 ~ b4*x4
y5 ~ b5*x5
# Allow unrestricted correlation between eta and covariates
a ~~ x1 + x2 + x3 + x4 + x5
x1 ~~ x2 + x3 + x4 + x5
x2 ~~ x3 + x4 + x5
x3 ~~ x4 + x5
x4 ~~ x5
# Constrain residual variances to be equal over time
y1 ~~ e1*y1
y2 ~~ e2*y2
y3 ~~ e3*y3
y4 ~~ e4*y4
y5 ~~ e5*y5
'
fe_semd.fit <- sem( model = fe_semd, 
                   data = dfw, 
                   estimator = "ML")
```

\small
```{r}
anova( fe_sem.fit, fe_semb.fit, fe_semc.fit, fe_semd.fit)
```
\normalsize

Keep in mind that a less parsimonious model (fewer degrees of freedom) can never fit worse than a more parsimonious one (more degrees of freedom). I.e., chance variations due to sampling error mean that adding constraints to a model will tend to always worsen fit, at least minimally. The question here is whether the improvement to fit by loosening constraints is meaningful or not. In the table above, we should not expect any meaningful improvements moving from `fe_sem.fit` to `fe_semd.fit`. Here, using simulated data, we have the luxury of knowing that any significant differences in $\chi^{2}$ are due to chance. With real data, it is up to the researcher to apply their best judgment and decide whether the results are plausible or not. 

## Measurement error {#measerr}

What if the observed variables are not measured perfectly? Then what we observe, call them $\tilde{x}_{t}$ and $\tilde{y}_{t}$ are composites of the true score we are after, i.e., $x_{t}$ and $y_{t}$, plus an additive measurement error portion:
\begin{align}
\tilde{x}_{t} & = x_{t} + \upsilon_{t}, \\
\tilde{y}_{t} & = y_{t} + \nu_{t}.
\end{align}
How does this affect our model? Well, first notice that measurement error in the dependent variable is typically less of a serious problem than measurement error in the independent variables. Let us assume again mean-centered variables so that we can ignore the intercept, and consider the following simple bivariate equation:
\begin{align}
y & = \beta x + \varepsilon
\end{align}
if $y$ is measured imperfectly and what we observe is $\tilde{y} = y + \nu$, then we can rewrite the equation as: 
\begin{align}
(\tilde{y} - \nu) & = \beta x + \varepsilon \\
\tilde{y} & = \beta x + \varepsilon + \nu.
\end{align}
The measurement error in $y$ just gets added to the regression error. As long as $\nu$ is uncorrelated with $x$, then the regression coefficient will be unbiased [@Pischke2007; @Wooldridge2009]. However, this will increase the error variance and thus make the estimates less precise. 

We will look at the effect of measurement error in the dependent variable using an example shortly. For now though, let us be safe in the knowledge that the coefficient of interest is likely unbiased, and concentrate on the more serious problem of error in the independent variable. 

The intuition behind the problem of measurement error in the independent variable(s) can be explained as follows. Take $\tilde{x} = x + \upsilon$ and substitute this into the equation for $y$: 
\begin{align}
y & = \beta x + \varepsilon \\
 & = \beta(\tilde{x} - \upsilon) + \varepsilon \\
 & = \beta\tilde{x} + (\varepsilon - \beta\upsilon).
\end{align}
Since $\tilde{x}$ is obviously correlated with $\upsilon$ (unless the variance of $\upsilon$ is so small so that the correlation is essentially negligible), then the composite error in this regression is also correlated with the independent variable and thus the estimated coefficient of $\beta$ will be biased. 

### The consequences of measurement error 

To demonstrate the effect of measurement error on the FE-SEM model, and then provide a strategy for dealing with measurement error in SEM, let us first modify the simulated dataset by generating multiple *indicators* of the independent and dependent variables that all measure the intended variable imprecisely. Returning to our panel data, we have three indicators of each the independent and dependent variable, per timepoint:  
\begin{align}
\tilde{x}_{kt} & = x_{t} + \upsilon_{kt}, \\
\tilde{y}_{kt} & = y_{t} + \nu_{kt}
\end{align}
where $k = 1, 2, 3$ and $t = 1, ..., T$. This is like repeatedly presenting a respondent with a multi-item scale designed to measure things like stress, depression, xenophobia, etc. over the course of a panel study. To create the observed indicators, a random amount of measurement error (ranging from $\{\sigma^{2}_{\upsilon_{k}}, \sigma^{2}_{\nu_{k}}\}\in \{1.0, 1.1, 1.2, 1.3, 1.4, 1.5\}$) was added to the true variables. The code for generating impercise indicators can be found in Appendix B. 

Let us first focus on the issue of imprecise measurements of the independent variable of interest and run the same FE-SEM model above, but this time we will use one of the measurement error sullied indicators, here $\tilde{x}_{1t}$, instead of the true independent variable, $x_{t}$. 

```{r}
fe_sem2 <- '
# Define individual effects variable 
a =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5
# Regressions, constrain coefficient to be equal over time
# Now the imprecisely measured indicator tilde{x}_kt
# instead of the true variable x_t
y1 ~ b*x11 
y2 ~ b*x12 
y3 ~ b*x13
y4 ~ b*x14
y5 ~ b*x15
# Allow unrestricted correlation between eta and covariates
a ~~ x11 + x12 + x13 + x14 + x15
x11 ~~ x12 + x13 + x14 + x15
x12 ~~ x13 + x14 + x15
x13 ~~ x14 + x15
x14 ~~ x15
# Constrain residual variances to be equal over time
y1 ~~ e*y1
y2 ~~ e*y2
y3 ~~ e*y3
y4 ~~ e*y4
y5 ~~ e*y5
'
fe_sem2.fit <- sem( model = fe_sem2, 
                    data = dfw, 
                    estimator = "ML")
```

Now, for the sake of brevity, let us look just at the estimated coefficients for $\beta$. 

```{r, echo=TRUE, output.lines=32:43}
summary( fe_sem2.fit)
```

Obviously, the estimated coefficient $\hat{\beta} =$ `r round( lavInspect( fe_sem2.fit, "list")[ 6, 14], 3)` is substantially smaller than the true population coefficient of $\beta = 0.3$. And the discrepancy is not just due to sampling error. In fact, we can derive the bias we are observing here. 

For a simple bivariate regression model, it is straightforward to quantify the bias due to measurement error. It will be 
\begin{align}
\Cov(y, \tilde{x}) & = \E[y \tilde{x}] \\
 & = \E[(\beta\tilde{x} + \varepsilon)\tilde{x}] \\
 & = \E[\beta\tilde{x}^{2} + \varepsilon \tilde{x}] \\
 & = \beta \Var(\tilde{x}) \\
\hat{\beta} & = \frac{\Cov(y, \tilde{x})}{\Var(\tilde{x})} \\
 & = \frac{\E[(\beta x + \varepsilon)(x + \upsilon)]}{\E[(x + \upsilon)^{2}]} \\
 & = \frac{\E[\beta x^{2} + \beta x \upsilon + \varepsilon x + \varepsilon \upsilon]}{\E[x^{2} + 2 x \upsilon + \upsilon^{2}]} \\
 & = \beta \frac{\Var(x)}{\Var(x) + \Var(\upsilon)}.
\end{align}
which results if we assume that $\E[x \upsilon] = 0$, $\E[x \varepsilon] = 0$, $\E[\tilde{x} \varepsilon] = 0$ and $\E[\varepsilon \upsilon] = 0$ [@Wooldridge2009]. However, the model we are interested is not a bivariate model, so what was the point of showing the this? For one, it points out that the bias will always move the estimated coefficient closer to 0, since $\Var(x) \le \Var(x) + \Var(\upsilon)$. This means positive effects will be biased downwards and negative effects biased upwards, always towards zero. This is why it is referred to as *attenuation bias*. Second, it will help to familiarize ourselves with this equation to better understand the one for the multivariate case. 

Indeed, the magnitude of the bias in a multivariate model is somewhat more complex to derive, but it will be 
\begin{align}
\hat{\beta} & = \beta \frac{\Var(\theta)}{\Var(\theta) + \Var(\upsilon)}
\end{align}
where $\theta$ is just the residual of a regression in which the true underlying variable is regressed on all other covariates. In this case, we need to regress $x_{t}$ on $\alpha_{1}$ and $\alpha_{2}$ for: $x_{t} = \gamma_{1}\alpha_{1} + \gamma_{2}\alpha_{2} + \theta_{t}$ [@Wooldridge2009, p. 318--320].

Normally it is not possible to reconstruct the bias since in cases where we have to rely on indicators, we would not know the true underlying variable. Furthermore, in the case of a fixed-effects model, the covariates are the unobserved time-invariant characteristics. However, because we are working with simulated data, we have everything we need. Going back to the results above, we can get the residuals of $x_{t}$ by either running a regression and saving the residuals, or we could skip a step and get them directly using the 'residual maker' matrix [@Ruettenauer2020] which is $\bm{M} = \bm{I} - \bm{A}(\bm{A}^{\intercal}\bm{A})^{-1}\bm{A}^{\intercal}$ and $\bm{A} = \begin{pmatrix}\bm{\alpha_{1}}, \bm{\alpha_{2}}\end{pmatrix}$ is the $n \times 2$ matrix of the covariates.

```{r}
# Make the n x n identity matrix
Id <- diag( n)

# n x 2 matrix of covariates a1 and a2
A <- matrix( c( rep( 1, n), dfw$a1, dfw$a2), 
             nrow = n, ncol = 3)

# The residual maker matrix M = I - A(A'A)^-1 A'
M <- Id - A %*% solve( t( A) %*% A) %*% t( A)

# Save the residuals, t for 'theta'
t <- M %*% dfw$x1

# Re-run the FE model from above with the 'true' 
# independent variable for the correct estimate for beta 
fe_sem.fit <- sem( model = fe_sem, data = dfw, estimator = "ML")

# The equation for the biased beta 
lavInspect( fe_sem.fit, "list")[ 6, 14]*
  (( var( t))/( var( t) + var( dfw$x11 - dfw$x1)))
```

```{r, echo=FALSE, results=FALSE}
b <- lavInspect( fe_sem.fit, "list")[ 6, 14]
vart <- var( t)
varu <- var( dfw$x11 - dfw$x1)
```

From this we can see that the biased estimate above of $\hat{\beta} =$ `r round( lavInspect( fe_sem2.fit, "list")[ 6, 14], 3)` roughly comes from $\beta \frac{\Var(\theta_{t})}{\Var(\theta_{t}) + \Var(\upsilon_{t})} = `r round( b, 3)` \frac{`r  round( vart, 3)`}{`r  round( vart + varu, 3)`}$ = `r round( b*(( vart)/( vart + varu)), 3)`; 'roughly' because the equation here is the population equation. Due to sampling error, the estimates will tend vary slightly.  

### Using latent variables to deal with measurement error 

The way we deal with measurement error in SEM is surprisingly similar to the logic of fixed-effects regression. Namely, if we have multiple cross-sectional observations of the underlying construct of interest, then we can define a latent variable that represents the common variance across those multiple variables. Contrast this with the use of longitudinal repeated measures to isolate the common variance across time. 

So, if we do in fact have multiple cross-sectional indicators for the underlying variables of interest, then we can partition them into an explained and unexplained portion:  
\begin{align}
x_{kt} & = \lambda_{kt}^{x}\xi_{t} + \delta_{kt}, \\
y_{kt} & = \lambda_{kt}^{y}\eta_{t} + \varepsilon_{kt},
\end{align}
where $x_{kt}$ and $y_{kt}$ are the $k^{th}$ indicators, $\xi_{t}$ and $\eta_{t}$ are latent factors representing the common variance across the cross-sectional repeated measures, and $\delta_{kt}$ and $\varepsilon_{kt}$ are the unexplained portions of $x_{t}$ and $y_{t}$, respectively. The latent factors are linked to the observed indicators through the factor loadings $\lambda_{kt}$. 

Thus, our FE regression equation changes from $y_{t} = \beta x_{t} + \alpha + \varepsilon_{t}$ to:
\begin{align}
\eta_{t} & = \beta \xi_{t} + \alpha + \zeta_{t}
\end{align}
where $\zeta_{t}$ represents the disturbance, in other words the residual of the latent dependent variable $\eta_{t}$. 

First, however, let us double-check that measurement error in the dependent variable only increases the error variance (thus also increasing standard errors and reducing $R^{2}$), but does not systematically bias the coefficients of interest. The next model uses the indicators of $x$ and specifies latent variables to represent the valid cross-sectional variance. The dependent variable in the model is one of the impercisely measured indicators of $y$.   

```{r}
fe_sem3 <- '
# Define individual effects variable 
a =~ 1*y11 + 1*y12 + 1*y13 + 1*y14 + 1*y15
# Measurement model for independent variables, xi 
xi1 =~ 1*x11 + x21 + x31 
xi2 =~ 1*x12 + x22 + x32
xi3 =~ 1*x13 + x23 + x33
xi4 =~ 1*x14 + x24 + x34
xi5 =~ 1*x15 + x25 + x35
# Regressions, constrain coefficient to be equal over time
y11 ~ b*xi1
y12 ~ b*xi2 
y13 ~ b*xi3
y14 ~ b*xi4
y15 ~ b*xi5
# Allow unrestricted correlation between eta and covariates
a ~~ xi1 + xi2 + xi3 + xi4 + xi5
xi1 ~~ xi2 + xi3 + xi4 + xi5
xi2 ~~ xi3 + xi4 + xi5
xi3 ~~ xi4 + xi5
xi4 ~~ xi5
# Constrain residual variances to be equal over time
y11 ~~ e*y11
y12 ~~ e*y12
y13 ~~ e*y13
y14 ~~ e*y14
y15 ~~ e*y15
'
fe_sem3.fit <- sem( model = fe_sem3, 
                    data = dfw, 
                    estimator = "ML")
```

```{r, output.lines=51:62}
summary( fe_sem3.fit)
```

The estimated coefficient here in model `fe_sem3.fit` is $\hat{\beta}_{y_{1t},\xi_{t}} =$ `r round( lavInspect( fe_sem3.fit, "list")[ 21, 14], 3)` which is very close to the estimated coefficient in the first, correctly specified model `fe_sem.fit`, where $\hat{\beta}_{y_{t},x_{t}} =$ `r round( lavInspect( fe_sem.fit, "list")[ 6, 14], 3)`. Notice, however, that the standard error of the estimate is substantially larger, with `r round( lavInspect( fe_sem3.fit, "list")[ 21, 15], 3)` in `fe_sem3.fit` vs. `r round( lavInspect( fe_sem.fit, "list")[ 6, 15], 3)` in `fe_sem.fit` in which $y$ was measured without error. The explained variance ($R^{2}$) in the dependent variable was also much higher in the first model: 

```{r}
lavInspect( fe_sem.fit, "r2")[ 1:5]
```

compared to the current model: 

```{r}
lavInspect( fe_sem3.fit, "r2")[ 1:5]
```

Finally, to see the benefits of removing measurement error from the dependent variable in terms of standard errors and $R^{2}$ statistics, we can specify a model with latent variables representing the valid cross-sectional variance in $y$. 

```{r}
fe_sem4 <- '
# Measurement model for dependent variable, n for eta
n1 =~ 1*y11 + y21 + y31
n2 =~ 1*y12 + y22 + y32
n3 =~ 1*y13 + y23 + y33
n4 =~ 1*y14 + y24 + y34
n5 =~ 1*y15 + y25 + y35
# Define individual effects variable 
a =~ 1*n1 + 1*n2 + 1*n3 + 1*n4 + 1*n5
# Measurement model for independent variables, xi 
xi1 =~ 1*x11 + x21 + x31 
xi2 =~ 1*x12 + x22 + x32
xi3 =~ 1*x13 + x23 + x33
xi4 =~ 1*x14 + x24 + x34
xi5 =~ 1*x15 + x25 + x35
# Regressions, constrain coefficient to be equal over time
n1 ~ b*xi1
n2 ~ b*xi2 
n3 ~ b*xi3
n4 ~ b*xi4
n5 ~ b*xi5
# Allow unrestricted correlation between eta and covariates
a ~~ xi1 + xi2 + xi3 + xi4 + xi5
xi1 ~~ xi2 + xi3 + xi4 + xi5
xi2 ~~ xi3 + xi4 + xi5
xi3 ~~ xi4 + xi5
xi4 ~~ xi5
# Constrain residual variances to be equal over time
n1 ~~ e*n1
n2 ~~ e*n2
n3 ~~ e*n3
n4 ~~ e*n4
n5 ~~ e*n5
'
fe_sem4.fit <- sem( model = fe_sem4, 
                    data = dfw, 
                    estimator = "ML")
```

```{r, output.lines=71:82}
summary( fe_sem4.fit)
```

Generally, the use of a latent variable to account for measurement error in the dependent variable will help smooth out chance variation in the estimated coefficient and increase model efficiency, i.e., make the estimated coefficient closer to the true parameter more often. The standard errors will tend to be smaller compared to a model in which the dependent variable still contains measurement error (e.g., `fe_sem4.fit`: `r round( lavInspect( fe_sem4.fit, "list")[ 36, 15], 3)` vs. `fe_sem3.fit`: `r round( lavInspect( fe_sem3.fit, "list")[ 21, 15], 3)`) and the model fit will be better.  

## Time-invariant predictors

What if we do not just want to just control for the effects of all time-invariant variables, but investigate some of them in detail? Many time-invariant variables, like sex, birth cohort, nationality, education, etc. can be interesting on their own. And typically, many of these variables are readily available in a given dataset. The traditional OLS-based FE model does not allow for this, as it wipes out the effect of *all* time-invariant variables, whether observed or not. 

In SEM, we can easily specify a type of *hybrid* FE/RE model [@Bollen2010] that allows us to control for time-invariant unobserved heterogeneity while also investigating the effects of specific observed time-invariant predictors.^[These types of models have become well known outside of SEM as well, see for example @Allison2011; @Schunck2013; @Bell2018.]

In the next example, we continue with the most complex model we have specified so far, `fe_sem4.fit` in which measurement error in both the independent and dependent variables is accounted for using latent variables. Now, we would like as well to specifically investigate the effect of $\alpha_{2}$ on the dependent variable. The equation for this model changes to: $\eta_{t} = \beta \xi_{t} + \alpha + \gamma \alpha_{2} + \zeta_{t}$. 

```{r eval=TRUE}
fe_sem5 <- '
# Measurement model for dependent variable, n for eta
n1 =~ 1*y11 + y21 + y31
n2 =~ 1*y12 + y22 + y32
n3 =~ 1*y13 + y23 + y33
n4 =~ 1*y14 + y24 + y34
n5 =~ 1*y15 + y25 + y35
# Define individual effects variable 
a =~ 1*n1 + 1*n2 + 1*n3 + 1*n4 + 1*n5
# Measurement model for independent variables, xi 
xi1 =~ 1*x11 + x21 + x31 
xi2 =~ 1*x12 + x22 + x32
xi3 =~ 1*x13 + x23 + x33
xi4 =~ 1*x14 + x24 + x34
xi5 =~ 1*x15 + x25 + x35
# Regressions, constrain coefficient to be equal over time
n1 ~ b*xi1 + g*a2
n2 ~ b*xi2 + g*a2
n3 ~ b*xi3 + g*a2
n4 ~ b*xi4 + g*a2
n5 ~ b*xi5 + g*a2
# Allow unrestricted correlation between eta and covariates
a ~~ xi1 + xi2 + xi3 + xi4 + xi5 + 0*a2
a2 ~~ xi1 + xi2 + xi3 + xi4 + xi5
xi1 ~~ xi2 + xi3 + xi4 + xi5
xi2 ~~ xi3 + xi4 + xi5
xi3 ~~ xi4 + xi5
xi4 ~~ xi5
# Constrain residual variances to be equal over time
n1 ~~ e*n1
n2 ~~ e*n2
n3 ~~ e*n3
n4 ~~ e*n4
n5 ~~ e*n5
'
fe_sem5.fit <- sem( model = fe_sem5, 
                    data = dfw, 
                    estimator = "ML")
```

Keep in mind, based on the DGP, the true parameters are $\beta = 0.3$ and $\gamma = 0.45$.

```{r, output.lines=71:87, eval=TRUE}
summary( fe_sem5.fit)
```

From this we can see that such a hybrid model is does a good job of estimating the coefficients of interest, with $\hat{\beta} =$ `r round( lavInspect( fe_sem5.fit, "list")[ 36, 14], 3)` (`r round( lavInspect( fe_sem5.fit, "list")[ 36, 15], 3)`) and $\hat{\gamma} =$ `r round( lavInspect( fe_sem5.fit, "list")[ 37, 14], 3)` (`r round( lavInspect( fe_sem5.fit, "list")[ 37, 15], 3)`). 

It is important, however, to realize that the unbiasedness of $\hat{\gamma}$ in this model is dependent on the assumption that $\E[\zeta | \bm{\xi_{t}}, \alpha_{2}] = 0$. In other words, the idiosyncratic error is mean independent of $\bm{\xi_{t}} = \xi_{1}, \xi_{2}, ..., \xi_{T}$ as well as $\alpha_{2}$. The first part is easier to accept because we are controlling for all potential time-invariant confounders that could induce a relationship between the independent variable and the error. The unbiasedness of $\hat{\gamma}$, on the other hand rests on the assumption that the time-invariant predictor is independent of the error. If $\alpha_{2}$ represented the respondent's intelligence and $\eta_{t}$ represented the respondent's income, for example, then $\hat{\gamma}$ would be biased if both were dependent on a third time-invariant variable, say level of schooling. For this reason, we need to treat the regression on a time-invariant predictor like any other regular multivariate regression model and look to include all plausible potential confounders as controls in the model, or turn to other methods, e.g., instrumental variables. 

\newpage 

# References {-}

<div id="refs"></div>